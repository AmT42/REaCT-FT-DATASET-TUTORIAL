# REaCT-FT-DATASET-TUTORIAL

This project was developed for a hackathon at Station F in Paris, hosted by Mistral and Cerebral Valley. The focus is on fine-tuning the Mistral-7b-Instruct-v0.3 with function-calling capabilities. This repository contains the implementation of ChemCrow with a Mistral backbone. The detailed steps and insights will be shared in an upcoming Medium article.

Once the fine-tuning dataset is created, refer to the [Mistral fine-tuning GitHub repository](https://github.com/mistralai/mistral-finetune). It is self-explanatory on how to fine-tune Mistral on the created dataset. Make sure to download the Mistral-7b-Instruct-v0.3 version to make it work.

## üë©‚Äçüíª Installation

```
python setup.py install 
```

```bibtex
@misc{REaCT-FT-DATASET-TUTORIAL,
    author = "Ahmet Celebi",
    year = "2024",
    url = "https://github.com/AmT42/REaCT-FT-DATASET-TUTORIAL",
    email = "ahmet_celebi@hotmail.fr",
    company = "DeltaWave"
}
```
# REaCT-FT-DATASET-TUTORIAL


